{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/extn0st/spark/Learning/SparkSQL/data\"\n",
    "file_path = data_path + \"/utilization.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format('csv').option(\"header\",True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------+--------+------------+\n",
      "|   event_date|server_id|min_temp|max_temp|temp_celcius|\n",
      "+-------------+---------+--------+--------+------------+\n",
      "|3/5/2019 8:06|      100|    0.57|    0.51|          47|\n",
      "|3/5/2019 8:11|      100|    0.47|    0.62|          43|\n",
      "|3/5/2019 8:16|      100|    0.56|    0.57|          62|\n",
      "|3/5/2019 8:21|      100|    0.57|    0.56|          50|\n",
      "|3/5/2019 8:26|      100|    0.35|    0.46|          43|\n",
      "|3/5/2019 8:31|      100|    0.41|    0.58|          48|\n",
      "|3/5/2019 8:36|      100|    0.57|    0.35|          58|\n",
      "|3/5/2019 8:41|      100|    0.41|     0.4|          58|\n",
      "|3/5/2019 8:46|      100|    0.53|    0.35|          62|\n",
      "|3/5/2019 8:51|      100|    0.51|     0.6|          45|\n",
      "|3/5/2019 8:56|      100|    0.32|    0.37|          47|\n",
      "|3/5/2019 9:01|      100|    0.62|    0.59|          60|\n",
      "|3/5/2019 9:06|      100|    0.66|    0.72|          57|\n",
      "|3/5/2019 9:11|      100|    0.54|    0.54|          44|\n",
      "|3/5/2019 9:16|      100|    0.29|     0.4|          47|\n",
      "|3/5/2019 9:21|      100|    0.43|    0.68|          66|\n",
      "|3/5/2019 9:26|      100|    0.49|    0.66|          65|\n",
      "|3/5/2019 9:31|      100|    0.64|    0.55|          66|\n",
      "|3/5/2019 9:36|      100|    0.42|     0.6|          42|\n",
      "|3/5/2019 9:41|      100|    0.55|    0.59|          63|\n",
      "+-------------+---------+--------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataframe into a View suitable for running spark queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"util\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use .sql method to pull required columns as in SQL from the view 'util' created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"select event_date as dt, server_id as sid from util limit 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|           dt|sid|\n",
      "+-------------+---+\n",
      "|3/5/2019 8:06|100|\n",
      "|3/5/2019 8:11|100|\n",
      "|3/5/2019 8:16|100|\n",
      "|3/5/2019 8:21|100|\n",
      "|3/5/2019 8:26|100|\n",
      "|3/5/2019 8:31|100|\n",
      "|3/5/2019 8:36|100|\n",
      "|3/5/2019 8:41|100|\n",
      "|3/5/2019 8:46|100|\n",
      "|3/5/2019 8:51|100|\n",
      "|3/5/2019 8:56|100|\n",
      "|3/5/2019 9:01|100|\n",
      "|3/5/2019 9:06|100|\n",
      "|3/5/2019 9:11|100|\n",
      "|3/5/2019 9:16|100|\n",
      "|3/5/2019 9:21|100|\n",
      "|3/5/2019 9:26|100|\n",
      "|3/5/2019 9:31|100|\n",
      "|3/5/2019 9:36|100|\n",
      "|3/5/2019 9:41|100|\n",
      "+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Filters and order by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|           dt|sid|\n",
      "+-------------+---+\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:16|100|\n",
      "|4/9/2019 1:11|100|\n",
      "|4/9/2019 1:06|100|\n",
      "|4/9/2019 1:01|100|\n",
      "|4/9/2019 0:56|100|\n",
      "|4/9/2019 0:51|100|\n",
      "|4/9/2019 0:46|100|\n",
      "|4/9/2019 0:41|100|\n",
      "|4/9/2019 0:36|100|\n",
      "|4/9/2019 0:31|100|\n",
      "|4/9/2019 0:26|100|\n",
      "|4/9/2019 0:21|100|\n",
      "|4/9/2019 0:16|100|\n",
      "|4/9/2019 0:11|100|\n",
      "|4/9/2019 0:06|100|\n",
      "|4/9/2019 0:01|100|\n",
      "|4/8/2019 9:56|100|\n",
      "|4/8/2019 9:51|100|\n",
      "|4/8/2019 9:46|100|\n",
      "+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"select event_date as dt, server_id as sid from util where server_id = 100 order by event_date desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_date', 'server_id', 'min_temp', 'max_temp', 'temp_celcius']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- server_id: string (nullable = true)\n",
      " |-- min_temp: string (nullable = true)\n",
      " |-- max_temp: string (nullable = true)\n",
      " |-- temp_celcius: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+--------+--------+------------+\n",
      "|   event_date|server_id|min_temp|max_temp|temp_celcius|\n",
      "+-------------+---------+--------+--------+------------+\n",
      "|3/5/2019 8:06|      100|    0.57|    0.51|          47|\n",
      "|3/5/2019 8:11|      100|    0.47|    0.62|          43|\n",
      "|3/5/2019 8:16|      100|    0.56|    0.57|          62|\n",
      "|3/5/2019 8:21|      100|    0.57|    0.56|          50|\n",
      "|3/5/2019 8:26|      100|    0.35|    0.46|          43|\n",
      "|3/5/2019 8:31|      100|    0.41|    0.58|          48|\n",
      "|3/5/2019 8:36|      100|    0.57|    0.35|          58|\n",
      "|3/5/2019 8:41|      100|    0.41|     0.4|          58|\n",
      "|3/5/2019 8:46|      100|    0.53|    0.35|          62|\n",
      "|3/5/2019 8:51|      100|    0.51|     0.6|          45|\n",
      "|3/5/2019 8:56|      100|    0.32|    0.37|          47|\n",
      "|3/5/2019 9:01|      100|    0.62|    0.59|          60|\n",
      "|3/5/2019 9:06|      100|    0.66|    0.72|          57|\n",
      "|3/5/2019 9:11|      100|    0.54|    0.54|          44|\n",
      "|3/5/2019 9:16|      100|    0.29|     0.4|          47|\n",
      "|3/5/2019 9:21|      100|    0.43|    0.68|          66|\n",
      "|3/5/2019 9:26|      100|    0.49|    0.66|          65|\n",
      "|3/5/2019 9:31|      100|    0.64|    0.55|          66|\n",
      "|3/5/2019 9:36|      100|    0.42|     0.6|          42|\n",
      "|3/5/2019 9:41|      100|    0.55|    0.59|          63|\n",
      "+-------------+---------+--------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Aggregeation using SQL , use groupby , having and order by clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|server_id|avg_temp|\n",
      "+---------+--------+\n",
      "|      149| 81.9645|\n",
      "|      148| 81.2747|\n",
      "|      147| 59.3039|\n",
      "|      146| 77.8002|\n",
      "|      145| 85.7492|\n",
      "|      144|   74.83|\n",
      "|      143| 53.4296|\n",
      "|      142| 77.8072|\n",
      "|      141| 64.0031|\n",
      "|      140| 74.5538|\n",
      "|      139| 78.8032|\n",
      "|      138| 51.0309|\n",
      "|      137| 81.8962|\n",
      "|      136| 67.9138|\n",
      "|      135| 58.8371|\n",
      "|      134| 60.4713|\n",
      "|      133| 82.9917|\n",
      "|      132| 60.0433|\n",
      "|      131| 68.8114|\n",
      "|      130|  63.061|\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"select server_id, avg(temp_celcius) as avg_temp\\\n",
    "                   from util \\\n",
    "                   group by server_id \\\n",
    "                   having avg_temp>50 \\\n",
    "                    order by 1 desc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another df to join to util df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/extn0st/spark/Learning/SparkSQL/data\"\n",
    "file_path_s = data_path + \"/server_name.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s=spark.read.format('csv').option(\"header\",True).load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s.createOrReplaceTempView(\"server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use joins in ANSI SQL format to join two df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+\n",
      "|           dt|sid|\n",
      "+-------------+---+\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "|4/9/2019 1:21|100|\n",
      "+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql = spark.sql(\"select u.event_date as dt, s.server_id as sid \\\n",
    "                    from util u \\\n",
    "                    inner join server s \\\n",
    "                   on s.server_id = u.server_id \\\n",
    "                   where u.server_id = 100 order by u.event_date desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
